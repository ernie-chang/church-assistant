import os
import re
import glob
import gc
from typing import List, Optional

import pandas as pd
from datetime import datetime
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.font_manager as fm

import google.generativeai as genai

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
FONT_PATH = os.path.join(CURRENT_DIR, 'fonts', 'NotoSansTC-Regular.ttf')
if os.path.exists(FONT_PATH):
    # å¼·åˆ¶åŠ å…¥å­—é«”åˆ° Matplotlib çš„å­—é«”ç®¡ç†å™¨
    fm.fontManager.addfont(FONT_PATH)
    # ç²å–è©²å­—é«”çš„æ­£å¼åç¨±
    custom_font_name = fm.FontProperties(fname=FONT_PATH).get_name()
    # è¨­å®šç‚ºå…¨åŸŸé è¨­å­—é«”
    plt.rcParams['font.family'] = custom_font_name
    # ä¿®æ­£è² è™Ÿé¡¯ç¤ºå•é¡Œ
    plt.rcParams['axes.unicode_minus'] = False
    print(f"âœ… å·²æˆåŠŸè¼‰å…¥å­—é«”: {custom_font_name}")
else:
    print(f"âŒ æ‰¾ä¸åˆ°å­—é«”æª”: {FONT_PATH}")
    # Mac å‚™æ¡ˆï¼šå¦‚æœæœ¬åœ°æ²’æ”¾å­—é«”ï¼Œå˜—è©¦ç”¨ Mac å…§å»ºå­—é«”é è¦½ (ä½†éƒ¨ç½²åˆ° Render æœƒå¤±æ•ˆ)
    plt.rcParams['font.family'] = 'Arial Unicode MS'

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
try:
    generation_config = {
    "temperature": 0,  # è¨­ç‚º 0 ç¢ºä¿å›ç­”ä¸€è‡´æ€§
}
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash', generation_config=generation_config)
except Exception as e:
    # å¦‚æœ API key æœªè¨­å®šæˆ–é€£ç·šå¤±æ•—ï¼Œå‰‡ model ç‚º None
    print(f"Gemini é…ç½®å¤±æ•—ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨: {e}")
    model = None


REGION_MAPPING = {
    "é«˜ä¸­å¤§å€": ["é«˜ä¸­ä¸€å€", "é«˜ä¸­äºŒå€"],
    "é’å¹´å¤§å€": ["é’å¹´ä¸€å€", "é’å¹´äºŒå€", "é’å¹´ä¸‰å€"], 
    "åœ‹ä¸­å¤§å€": ["åœ‹ä¸­ä¸€å€", "åœ‹ä¸­äºŒå€"], 
}
NUMERIC_COLUMNS_CANDIDATES = ["ä¸»æ—¥", "ç¦±å‘Š", "å®¶å‡ºè¨ª", "å®¶å—è¨ª", "å°æ’", "æ™¨èˆˆ", "ç¦å‡ºè¨ª"]
ATTENDANCE_COLS = ['ä¸»æ—¥', 'ç¦±å‘Š', 'å°æ’', 'æ™¨èˆˆ']

# -----------------------------------------------------------
# RAG æ ¸å¿ƒå‡½å¼
# -----------------------------------------------------------

def _load_recent_summary_data(reports_dir_summary: str, weeks: int = 5) -> Optional[pd.DataFrame]:
    """è¼‰å…¥æ‰€æœ‰ç¸½çµå ±è¡¨ï¼Œä¸¦åƒ…ä¿ç•™æœ€è¿‘ N é€±çš„æ•¸æ“šã€‚ (ä¿æŒä¸è®Š)"""
    try:
        df_all = aggregate_reports(reports_dir_summary)
        if df_all.empty: return None
        unique_dates = df_all["é€±æœ«æ—¥"].dropna().unique()
        recent_dates = pd.Series(unique_dates).sort_values(ascending=False).head(weeks)
        df_recent = df_all[df_all["é€±æœ«æ—¥"].isin(recent_dates)].copy()
        df_recent.sort_values("é€±æœ«æ—¥", inplace=True)
        return df_recent
    except RuntimeError:
        return None


def _load_filtered_raw_personal_data(reports_dir_excel: str, weeks: int = 5) -> Optional[pd.DataFrame]:
    """
    è¼‰å…¥å€‹äººåŸå§‹æ•¸æ“šï¼Œä¸¦éæ¿¾äº”é€±å…§å®Œå…¨æ²’å‡ºç¾çš„è–å¾’ã€‚
    """
    pattern = os.path.join(reports_dir_excel, "attend_*.xls*")
    file_paths = glob.glob(pattern)
    
    if not file_paths:
        print(f"DEBUG: åœ¨ {reports_dir_excel} æ‰¾ä¸åˆ° attend_*.xlsx æª”æ¡ˆ")
        return None
        
    # å»ºç«‹æª”æ¡ˆæ¸…å–®ä¸¦æ’åº
    file_info = []
    for f in file_paths:
        dt = parse_week_end_date_from_filename(f)
        if dt:
            file_info.append((dt, f))
    
    # æŒ‰æ—¥æœŸç”±æ–°åˆ°èˆŠæ’åºï¼Œå–å‰ N é€±
    file_info.sort(key=lambda x: x[0], reverse=True)
    recent_files = file_info[:weeks]
    
    if not recent_files:
        print("DEBUG: æ‰¾ä¸åˆ°æ—¥æœŸç¬¦åˆæ ¼å¼çš„ Excel æª”æ¡ˆ")
        return None
        
    all_data = []
    attendance_cols = ['ä¸»æ—¥', 'ç¦±å‘Š', 'å°æ’', 'æ™¨èˆˆ']
    
    for dt, file_path in recent_files:
        try:
            df = pd.read_excel(file_path)
            df.columns = [str(c).strip() for c in df.columns]
            
            if 'å§“å' not in df.columns: continue
            
            # é¸å–å¿…è¦æ¬„ä½ä¸¦è£œé›¶
            available_cols = ['å§“å', 'å€åˆ¥'] + [c for c in attendance_cols if c in df.columns]
            temp_df = df[available_cols].copy()
            
            for c in attendance_cols:
                if c in temp_df.columns:
                    temp_df[c] = pd.to_numeric(temp_df[c], errors='coerce').fillna(0).astype(int)
                else:
                    temp_df[c] = 0
            
            temp_df['æ—¥æœŸ'] = dt.strftime('%Y/%m/%d')
            all_data.append(temp_df)
        except Exception as e:
            print(f"è®€å– {file_path} å‡ºéŒ¯: {e}")
            continue

    if not all_data: return None

    df_total = pd.concat(all_data, ignore_index=True)

    # ğŸš¨ éæ¿¾ï¼šåªä¿ç•™äº”é€±å…§è‡³å°‘æœ‰ä¸€æ¬¡å‡ºå¸­çš„äºº
    person_sum = df_total.groupby('å§“å')[attendance_cols].transform('sum').sum(axis=1)
    df_filtered = df_total[person_sum > 0].copy()

    # å›å‚³æ•´ç†å¾Œçš„æµæ°´å¸³ï¼Œæ–¹ä¾¿ Gemini æ¯”å°
    return df_filtered[['æ—¥æœŸ', 'å€åˆ¥', 'å§“å', 'ä¸»æ—¥', 'ç¦±å‘Š', 'å°æ’', 'æ™¨èˆˆ']].sort_values(['æ—¥æœŸ', 'å€åˆ¥'], ascending=[False, True])


def _generate_rag_context(reports_dir_summary: str, reports_dir_excel: str) -> str:
    """
    ç”Ÿæˆè®“ Gemini é–±è®€çš„çŸ¥è­˜åº«å…§å®¹ã€‚
    """
    df_summary = _load_recent_summary_data(reports_dir_summary, weeks=5)
    df_personal = _load_filtered_raw_personal_data(reports_dir_excel, weeks=5)
    
    context = ""
    
    if df_summary is not None:
        context += "### [1. ç¸½çµå ±è¡¨æ•¸æ“š]\n"
        context += "é€™æ˜¯å„å€åˆ¥çš„å½™ç¸½æ•¸æ“šï¼Œé©åˆå›ç­”æ•´é«”è¶¨å‹¢å•é¡Œã€‚\n"
        context += df_summary.to_markdown(index=False) + "\n\n"
        
    if df_personal is not None:
        context += "### [2. å€‹äººåŸå§‹é»åæ˜ç´°]\n"
        context += "é€™æ˜¯æ¯å€‹äººåœ¨æ¯ä¸€é€±çš„å‡ºå¸­ç‹€æ³ï¼ˆ1=å‡ºå¸­, 0=ç¼ºå¸­ï¼‰ã€‚å¯ç”¨æ–¼è·¨é€±æ¯”å°åå–®ã€‚\n"
        context += df_personal.to_markdown(index=False) + "\n"
    else:
        context += "### [âš ï¸ æ³¨æ„]ï¼šç›®å‰ç„¡æ³•è®€å–å€‹äºº Excel è³‡æ–™ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆåç¨±æ˜¯å¦ç‚º attend_YYYY-MM-DD.xlsxã€‚\n"
        
    return context

GLOBAL_RAG_CONTEXT = "æ•¸æ“šåˆå§‹åŒ–ä¸­ï¼Œè«‹ç¨å€™..."

# ... (ä¿ç•™åŸæœ‰çš„å­—é«”è¨­å®šã€æ¨¡å‹è¨­å®š) ...

def update_global_rag_context(reports_dir_summary: str, reports_dir_excel: str):
    """
    æ‰‹å‹•è§¸ç™¼ï¼šé‡æ–°è®€å– Excel ä¸¦æ›´æ–°å…¨å±€å¿«å–æ–‡å­—ã€‚
    """
    global GLOBAL_RAG_CONTEXT
    print("ğŸ”„ æ­£åœ¨é‡æ–°æ§‹å»º RAG çŸ¥è­˜åº«å¿«å–...")
    try:
        # å‘¼å«æ‚¨åŸæœ‰çš„ generate å‡½å¼å–å¾—æ–‡å­—
        new_context = _generate_rag_context(reports_dir_summary, reports_dir_excel)
        GLOBAL_RAG_CONTEXT = new_context
        print(f"âœ… çŸ¥è­˜åº«å¿«å–æ›´æ–°å®Œæˆ (å­—æ•¸: {len(GLOBAL_RAG_CONTEXT)})")
        gc.collect()
    except Exception as e:
        print(f"âŒ å¿«å–æ›´æ–°å¤±æ•—: {e}")

# -----------------------------------------------------------
# ç¸½ RAG éŸ¿æ‡‰ç”Ÿæˆå‡½å¼ (çµ±ä¸€è™•ç†æ‰€æœ‰æŸ¥è©¢)
# -----------------------------------------------------------
def generate_rag_response(reports_dir_summary: str, reports_dir_excel: str, query: str) -> str:
    """
    çµ±ä¸€ RAG å‡½å¼ï¼šç”Ÿæˆä¸Šä¸‹æ–‡ä¸¦å‚³éçµ¦ Gemini é€²è¡Œæ¨ç†ã€‚
    """
    if not model:
        return "âŒ RAG åŠŸèƒ½æœªå•Ÿç”¨ï¼Œè«‹æª¢æŸ¥ Gemini API Key è¨­å®šã€‚"

    # 1. ç²å–æ‰€æœ‰æª”æ¡ˆæ¿ƒç¸®æˆçš„æ ¸å¿ƒæ•¸æ“šä¸Šä¸‹æ–‡
    rag_context = GLOBAL_RAG_CONTEXT
    
    # 2. æº–å‚™ç³»çµ±æç¤º
    system_prompt = f"""
    ä½ æ˜¯ä¸€å€‹æ™ºæ…§çš„æ•™æœƒæ•¸æ“šåˆ†ææ©Ÿå™¨äººã€‚ä½ çš„ç›®æ¨™æ˜¯æ ¹æ“šç”¨æˆ¶çš„å•é¡Œå’Œä¸‹æ–¹æä¾›çš„ã€RAG æ•¸æ“šçŸ¥è­˜åº«ã€ä¾†ç”Ÿæˆç²¾ç¢ºã€ç°¡æ½”ä¸”æœ‰æ¢ç†çš„ç­”æ¡ˆã€‚
    
    æ•¸æ“šæ¬„ä½èªªæ˜ï¼š
    - A å€å¡Šç”¨æ–¼å›ç­”ç¸½çµè¶¨å‹¢å’Œå€åˆ¥æ¯”è¼ƒå•é¡Œã€‚
    - B å€å¡Šæ˜¯**åŸå§‹çš„ã€æœªèšåˆçš„å€‹äººæ•¸æ“š**ï¼Œå¯ç”¨æ–¼å›ç­”**ä»»ä½•**å€‹äººç›¸é—œå•é¡Œï¼ŒåŒ…æ‹¬è·¨é€±æ¯”è¼ƒï¼ˆä¾‹å¦‚ï¼šä¸Šé€±æœ‰ä¾†é€™é€±æ²’ä¾†çš„äººã€æŸä½è–å¾’åœ¨äº”é€±å…§çš„å‡ºå¸­è¶¨å‹¢ï¼‰ã€‚
    
    è«‹åˆ©ç”¨æä¾›çš„æ•¸æ“šçŸ¥è­˜åº«é€²è¡Œåˆ†æå’Œå›ç­”ã€‚
    """
    
    # 3. çµåˆä¸Šä¸‹æ–‡å’Œç”¨æˆ¶æŸ¥è©¢
    full_prompt = f"{system_prompt}\n\n{rag_context}\n\n---\n\nç”¨æˆ¶å•é¡Œï¼š{query}"

    try:
        # 4. å‘¼å« Gemini
        response = model.generate_content(full_prompt)
        gc.collect()
        return response.text
    except Exception as e:
        gc.collect()
        return f"âŒ RAG è™•ç†å¤±æ•— (Gemini API éŒ¯èª¤): {e}"

def parse_week_end_date_from_filename(filename: str) -> Optional[datetime]:
    """
    å¾æª”åæå–æ—¥æœŸã€‚
    æ”¯æ´æ ¼å¼: attend_2025-12-08.xlsx æˆ– summary_2025-12-08.txt
    """
    base_name = os.path.basename(filename)
    # åŒ¹é… YYYY-MM-DD æ ¼å¼
    date_match = re.search(r"(\d{4})-(\d{1,2})-(\d{1,2})", base_name)
    if date_match:
        try:
            return datetime.strptime(date_match.group(0), "%Y-%m-%d")
        except ValueError:
            return None
    return None


def _clean_table_headers(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = [str(col).strip() for col in df.columns]
    return df


def _coerce_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:
    """å¼·åˆ¶è½‰æ›å‡ºå¸­æ¬„ä½ç‚ºæ•¸å­—"""
    for column_name in NUMERIC_COLUMNS_CANDIDATES:
        if column_name in df.columns:
            df[column_name] = pd.to_numeric(df[column_name], errors="coerce").fillna(0)
    return df


def read_single_report(file_path: str) -> Optional[pd.DataFrame]:
    week_end_date = parse_week_end_date_from_filename(file_path)
    if week_end_date is None:
        print(f"âš  ç„¡æ³•å¾æª”åè§£ææ—¥æœŸ: {os.path.basename(file_path)}ï¼Œå·²ç•¥é")
        return None

    try:
        dataframe = pd.read_excel(file_path, engine="openpyxl")
    except Exception as e:
        print(f"âš  ç„¡æ³•è®€å–ç¸½çµå ±è¡¨ {file_path}: {e}")
        return None

    if dataframe is None:
        print(f"âš  ç„¡æ³•è®€å–å ±è¡¨: {file_path}")
        return None

    dataframe = _clean_table_headers(dataframe)
    
    if "å€åˆ¥" not in dataframe.columns:
        print(f"âš  ç¸½çµå ±è¡¨ç¼ºå°‘å¿…è¦æ¬„ä½ 'å€åˆ¥': {file_path}ï¼Œå·²ç•¥é")
        return None

    dataframe = _coerce_numeric_columns(dataframe)
    dataframe["é€±æœ«æ—¥"] = week_end_date

    keep_columns = ["å€åˆ¥", "é€±æœ«æ—¥"] + [
        col for col in NUMERIC_COLUMNS_CANDIDATES if col in dataframe.columns
    ]
    return dataframe[keep_columns]


def _is_summary_text(value: object) -> bool:
    if not isinstance(value, str):
        return False
    return "ç¸½è¨ˆ" in value or "åˆè¨ˆ" in value


def _remove_summary_rows(df: pd.DataFrame) -> pd.DataFrame:
    if "å€åˆ¥" not in df.columns:
        return df
    mask_summary = df["å€åˆ¥"].apply(_is_summary_text)
    return df[~mask_summary].copy()


def aggregate_reports(reports_dir: str) -> pd.DataFrame:
    pattern = os.path.join(reports_dir, "*.xls*") 
    file_paths = sorted(glob.glob(pattern))
    
    if not file_paths:
        raise RuntimeError(f"åœ¨è³‡æ–™å¤¾ '{reports_dir}' ä¸­æ‰¾ä¸åˆ°å ±è¡¨æª”æ¡ˆã€‚")

    combined: List[pd.DataFrame] = []
    processed_count = 0
    for path in file_paths:
        report_df = read_single_report(path)
        if report_df is not None:
            combined.append(report_df)
            processed_count += 1
            
    if not combined:
        raise RuntimeError("æ²’æœ‰ä»»ä½•å¯ç”¨çš„å ±è¡¨è³‡æ–™ã€‚")

    all_data = pd.concat(combined, ignore_index=True)

    all_data = _remove_summary_rows(all_data)
    all_data.sort_values("é€±æœ«æ—¥", inplace=True)

    unique_weeks = all_data["é€±æœ«æ—¥"].dropna().unique()
    print(f"ğŸ“¦ å·²è®€å– {processed_count}/{len(file_paths)} ä»½ç¸½çµå ±è¡¨ï¼›é€±æ•¸: {len(unique_weeks)} ({', '.join(pd.Series(unique_weeks).dt.strftime('%Y/%m/%d'))})")

    return all_data


def build_region_timeseries(all_reports: pd.DataFrame, region_name: str) -> pd.DataFrame:
    """
    æ ¹æ“šåç¨± (ç¸½è¨ˆ, å€åˆ¥/å°å€, æˆ–å¤§å€) å»ºç«‹æ™‚é–“åºåˆ—æ•¸æ“šã€‚
    """
    
    if region_name == "ç¸½è¨ˆ":
        region_df = all_reports.copy()
    
    # ğŸš¨ ä¿®æ­£: è™•ç†å¤§å€åç¨± (å³åœ¨ REGION_MAPPING ä¸­çš„ Key)
    elif region_name in REGION_MAPPING:
        subdistricts = REGION_MAPPING[region_name]
        # éæ¿¾å‡ºå±¬æ–¼è©²å¤§å€çš„æ‰€æœ‰å°å€æ•¸æ“š
        region_df = all_reports[all_reports["å€åˆ¥"].isin(subdistricts)].copy()
        print(f"   -> åŒ¯ç¸½ {region_name}: åŒ…å« {', '.join(subdistricts)}")
        
    # è™•ç†å°å€åç¨± (å³åœ¨ 'å€åˆ¥' æ¬„ä½ä¸­çš„å€¼)
    else:
        region_df = all_reports[all_reports["å€åˆ¥"] == region_name].copy()
    
    
    if region_df.empty:
        return pd.DataFrame()

    aggregation_columns = [col for col in NUMERIC_COLUMNS_CANDIDATES if col in region_df.columns]
    
    # åŸ·è¡Œåˆ†çµ„åŠ ç¸½ (å¦‚æœæ˜¯å¤§å€æˆ–ç¸½è¨ˆï¼Œå‰‡æœƒå°‡å¤šå€‹å°å€çš„æ•¸æ“šåŠ ç¸½)
    ts = region_df.groupby("é€±æœ«æ—¥")[aggregation_columns].sum().sort_index()

    # è¨ˆç®—ç¸½å‡ºè¨ª (ä½¿ç”¨ API æ¬„ä½åç¨±)
    gospel = ts["ç¦å‡ºè¨ª"] if "ç¦å‡ºè¨ª" in ts.columns else 0
    home = ts["å®¶å‡ºè¨ª"] if "å®¶å‡ºè¨ª" in ts.columns else 0
    ts["ç¸½å‡ºè¨ª"] = gospel + home
        
    return ts


def _format_date_axis(ax, dates=None):
    if dates is not None:
        ax.set_xticks(pd.Index(dates))
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y/%m/%d"))
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", fontsize=11)
    ax.tick_params(axis="y", labelsize=11)
    ax.margins(y=0.15)
    ax.grid(True, alpha=0.3)


def _annotate_series(ax, x_index: pd.Index, y_series: pd.Series, fontsize: int = 12):
    for x, y in zip(x_index, y_series):
        if y > 0:
            ax.annotate(
                f"{int(y)}",
                (x, y),
                textcoords="offset points",
                xytext=(0, 10),
                ha="center",
                fontsize=fontsize,
                bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="none", alpha=0.8),
                zorder=3,
                clip_on=False,
            )

def _finalize_plot(plt_obj, output_path):
    """ğŸš¨ çµ±ä¸€è™•ç†åœ–è¡¨è¼¸å‡ºèˆ‡è¨˜æ†¶é«”æ¸…ç†"""
    plt_obj.tight_layout()
    # ğŸš¨ é™ä½ DPI ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨èˆ‡æª”æ¡ˆå¤§å° (80-90 é©åˆæ‰‹æ©Ÿé¡¯ç¤º)
    plt_obj.savefig(output_path, dpi=85) 
    plt_obj.clf()
    plt_obj.close('all')
    gc.collect() # ğŸ’¡ å¼·åˆ¶åƒåœ¾å›æ”¶

def plot_attendance(region_name: str, ts: pd.DataFrame, output_dir: str) -> None:
    # Only keep the last 5 weeks for plotting
    ts = ts.tail(5)
    if ts.empty or ts.sum().sum() == 0:
        return
        
    plt.figure(figsize=(10, 6))
    ax = plt.gca()

    # ç¹ªåœ–æ™‚ä½¿ç”¨çš„æ¬„ä½åç¨±ï¼Œè«‹æ³¨æ„é€™è£¡ä»ä½¿ç”¨ API åŸå§‹æ¬„ä½å
    columns_to_plot = [
        ("ä¸»æ—¥", "ç•¶å‘¨ä¸»æ—¥äººæ•¸", "red", "-"),
        ("å°æ’", "å°æ’äººæ•¸", "gold", "-"),
        ("æ™¨èˆˆ", "æ™¨èˆˆäººæ•¸", "green", "-"),
    ]

    plotted_any = False
    for column_key, label_text, color, linestyle in columns_to_plot:
        if column_key in ts.columns and ts[column_key].sum() > 0:
            ax.plot(ts.index, ts[column_key], label=label_text, color=color, linestyle=linestyle, marker="o", markersize=5, linewidth=2)
            _annotate_series(ax, ts.index, ts[column_key], fontsize=12)
            plotted_any = True

    if not plotted_any:
        print(f"âš  {region_name} æ²’æœ‰å¯ç¹ªè£½çš„å‡ºå¸­ç›¸é—œæ•¸æ“š")
        plt.close()
        return

    ax.set_title(f"{region_name} - å¬æœƒç”Ÿæ´»äººæ•¸è¶¨å‹¢ (è¿‘äº”é€±)")
    ax.set_xlabel("æ—¥æœŸ")
    ax.set_ylabel("äººæ•¸")
    ax.legend(loc="upper left")
    _format_date_axis(ax, dates=ts.index)

    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{region_name}_attendance.png")
    _finalize_plot(plt, output_path)
    print(f"âœ… å·²è¼¸å‡º {output_path}")


def plot_burden(region_name: str, ts: pd.DataFrame, output_dir: str) -> None:
    # Only keep the last 5 weeks for plotting
    ts = ts.tail(5)
    if ts.empty or ts.sum().sum() == 0:
        return
        
    plt.figure(figsize=(10, 6))
    ax = plt.gca()

    plotted_any = False
    if "ç¦±å‘Š" in ts.columns and ts["ç¦±å‘Š"].sum() > 0:
        ax.plot(ts.index, ts["ç¦±å‘Š"], label="ç¦±å‘Šäººæ•¸", color="#00aaff", marker="o", markersize=5, linewidth=2)
        _annotate_series(ax, ts.index, ts["ç¦±å‘Š"], fontsize=12)
        plotted_any = True
        
    if "ç¸½å‡ºè¨ª" in ts.columns and ts["ç¸½å‡ºè¨ª"].sum() > 0: 
        ax.plot(ts.index, ts["ç¸½å‡ºè¨ª"], label="ç¸½å‡ºè¨ªäººæ•¸", color="#0044aa", marker="o", markersize=5, linewidth=2)
        _annotate_series(ax, ts.index, ts["ç¸½å‡ºè¨ª"], fontsize=12)
        plotted_any = True
        
    if "å®¶å—è¨ª" in ts.columns and ts["å®¶å—è¨ª"].sum() > 0: # ç¸½çµå ±è¡¨ä¸­çš„ 'å®¶å—è¨ª'
        ax.plot(ts.index, ts["å®¶å—è¨ª"], label="å®¶å—è¨ªäººæ•¸", color="#66ccff", marker="o", markersize=5, linewidth=2)
        _annotate_series(ax, ts.index, ts["å®¶å—è¨ª"], fontsize=12)
        plotted_any = True

    if not plotted_any:
        print(f"âš  {region_name} æ²’æœ‰å¯ç¹ªè£½çš„è² æ“”ç›¸é—œæ•¸æ“š")
        plt.close()
        return

    ax.set_title(f"{region_name} - è² æ“”é ˜å—ç¨‹åº¦è¶¨å‹¢ (è¿‘äº”é€±)")
    ax.set_xlabel("æ—¥æœŸ")
    ax.set_ylabel("äººæ•¸")
    ax.legend(loc="upper left")
    _format_date_axis(ax, dates=ts.index)

    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{region_name}_burden.png")
    _finalize_plot(plt, output_path)
    print(f"âœ… å·²è¼¸å‡º {output_path}")


def generate_region_charts(all_reports: pd.DataFrame, region_name: str, output_dir: str) -> None:
    """ç”ŸæˆæŒ‡å®šåç¨± (ç¸½è¨ˆ, å¤§å€, æˆ–å°å€) çš„åœ–è¡¨"""
    ts = build_region_timeseries(all_reports, region_name)
    if ts.empty:
        print(f"âš  æ‰¾ä¸åˆ° {region_name} çš„è³‡æ–™ï¼Œç„¡æ³•ç¹ªåœ–")
        return
    
    plot_attendance(region_name, ts, output_dir)
    plot_burden(region_name, ts, output_dir)


if __name__ == "__main__":
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    reports_dir = os.path.join(base_dir, "reports_summary") 
    charts_output_dir = os.path.join(base_dir, "charts")

    try:
        df_reports = aggregate_reports(reports_dir)
        
        if "å€åˆ¥" not in df_reports.columns:
            raise RuntimeError("åŒ¯ç¸½å¾Œçš„è³‡æ–™ç¼ºå°‘ 'å€åˆ¥' æ¬„ä½ï¼Œç„¡æ³•åˆ†å€ç”Ÿæˆåœ–è¡¨ã€‚")

        # --- 1. ç”Ÿæˆ 'ç¸½è¨ˆ' åœ–è¡¨ ---
        generate_region_charts(df_reports, "ç¸½è¨ˆ", charts_output_dir)
        
        # --- 2. ç”Ÿæˆæ‰€æœ‰è‡ªå®šç¾©çš„ 'å¤§å€' åœ–è¡¨ (å¦‚: é«˜ä¸­å¤§å€, é’å¹´å¤§å€) ---
        print("\n--- ğŸŒ é–‹å§‹ç”Ÿæˆå¤§å€åœ–è¡¨ ---")
        for region_name in REGION_MAPPING.keys():
            generate_region_charts(df_reports, region_name, charts_output_dir)

        # --- 3. ç”Ÿæˆæ‰€æœ‰ 'å°å€' åœ–è¡¨ (å³ 'å€åˆ¥' æ¬„ä½ä¸­çš„ç¨ç«‹åç¨±) ---
        print("\n--- ğŸ’  é–‹å§‹ç”Ÿæˆå°å€åœ–è¡¨ ---")
        # åƒ…è¿­ä»£é‚£äº›æ²’æœ‰è¢«åŒ…å«åœ¨ REGION_MAPPING ä¸­çš„ç¨ç«‹å°å€ï¼Œæˆ–æ‰€æœ‰å°å€
        all_unique_districts = df_reports["å€åˆ¥"].dropna().unique()
        
        # ç‚ºäº†é¿å…é‡è¤‡ï¼Œæˆ‘å€‘å¯ä»¥é¸æ“‡åªç”Ÿæˆæœªè¢«æ­¸é¡åˆ°å¤§å€çš„å°å€ï¼Œæˆ–è€…ç”Ÿæˆæ‰€æœ‰å°å€
        # é€™è£¡é¸æ“‡ç”Ÿæˆæ‰€æœ‰çš„å°å€åœ–è¡¨ (å³ä½¿å®ƒè¢«æ­¸é¡åˆ°å¤§å€)ï¼Œä»¥æä¾›æœ€ç´°ç¯€çš„è¦–åœ–
        for subdistrict in sorted(all_unique_districts):
             # æ’é™¤ç¸½è¨ˆåˆ— (å¦‚æœä¹‹å‰æ²’è¢«ç§»é™¤çš„è©±)
            if not _is_summary_text(subdistrict):
                generate_region_charts(df_reports, str(subdistrict), charts_output_dir)
            
    except RuntimeError as e:
        print(f"âŒ åŸ·è¡Œåœ–è¡¨ç”Ÿæˆå¤±æ•—: {e}")